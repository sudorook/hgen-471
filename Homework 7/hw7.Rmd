---
title: "Problem Set 7"
author: "Ansel George"
output: pdf_document
---

```{r,message=F}
library(ggplot2)
library(tidyr)
library(dplyr)
library(readr)

set.seed(10)
```
# Homework 7

## Problem 1
*We are deriving a new test for pathway (SNP set) analysis. Suppose a pathway
is assigned $n$ SNPs, with p-values $p_1 , \hdots , p_n$. Our test is based on
one-sided $Z$-scores of these SNPs, $Z_1 , \hdots, Z_n$. The test statistic is
$Z = \sum_{i=1}^n Z_i$.*

### (a)
*Suppose the SNPs are all independent (not in LD), what is the null distribution
of $Z$?*

\begin{align}
E[Z] &= E[\sum_{i=1}^n Z_i] \\
  &= \sum_{i=1}^n E[Z_i] \\
  &= \sum_{i=1}^n 0 \\
  &= 0
\end{align}

If the Z-scores are independent, then they are completely uncorrelated.

\begin{align}
Var[Z] &= Var[\sum_{i=1}^n Z_{i}] \\
  &= \sum_{i=1}^n Var[Z_{i}] \\
  &= \sum_{i=1}^n 1 \\
  &= n
\end{align}

The test statistic $Z$ is distributed $\sim N(0, n)$.

### (b)
*Now suppose the first two SNPs are in perfect LD, what is the null
distribution of $Z$?*

\begin{align}
E[Z] &= E[\sum_{i=1}^n Z_i] \\
  &= \sum_{i=1}^n E[Z_i] \\
  &= \sum_{i=1}^n 0 \\
  &= 0
\end{align}

If Z-scores are in perfect LD, then their covariance is 1.

\begin{align}
r &= \frac{Cov(Z_i,Z_j)}{\sqrt{Var(Z_i)Var(Z_j)}} \\
1 &= \frac{Cov(Z_i,Z_j)}{1 * 1} \\
\implies Cov(Z_i,Z_j) &= 1
\end{align}

\begin{align}
Var[Z] &= Var[\sum_{i=1}^n Z_i] \\
  &= \sum_{i=1}^n Var[Z_i] + \sum_{j \neq i}^n\sum_{i=1}^n Cov[Z_i,Z_j] \\
  &= \sum_{i=1}^n 1 + \sum_{j \neq i}^n\sum_{i=1}^n 1 \\
  &= n + n(n-1) \\
  &= n^2
\end{align}


### (c)
*Suppose we have a pathway of $10$ SNPs, $p_1 = p_2 = 0.01$, $p_3 = p_4 = p_5 =
0.2$ and $p_6 = \hdots = p_{10} = 0.75$. What is the p-value at the pathway
level in the two scenarios above? Under which scenario, the pathway is more
significant, and why?*_

```{r}
p <- c(.01, .01, .2, .2, .2, .75, .75, .75, .75, .75)

# Association approach
z <- qnorm(p, 0, 1) # convert p-values to z-scores

pnorm(sum(z), 0, sqrt(length(z)))*2 # no linkage, 2-tailed
pnorm(sum(z), 0, sqrt(length(z)^2))*2 # full linkage, 2-tailed
```

The pathway under the no linkage model is more significant ($p=0.2288673$),
than under the full linkage model ($p=0.7035661$). This is understandable given
that the variance under the full linkage model is the square of that under no
linkage, so the latter distribution has much greater spread. The greater the
spread, the larger the deviations needed to obtain lower p-values.


### (d)
*A related test is $T = \sum_i Z_i^2$. What is the null distribution of this
test? And compare the two methods, using $Z$ or $Z^2$ , which one do you
prefer, and why?*

$Z_i^2$ follows a $\chi^2$ distribution, so $T$ is the sum of $\chi^2$
statistics.

If SNPs are independent, then $T$ also follows a $\chi^2$ distribution with
degrees of freedom equal to the total number of SNPs, but if not, then the
null distribution is undefined. It will need to be simulated by permuting the
data.

The $Z$ method is more straightforward and computationally simpler, but it
requires that the linkage data be already known in order to specify the
variance of the $\sum_i Z$ distribution. The $Z^2$ method is more flexible in
that it allows for significance testing given a set of p-values without the
need for accompanying linkage data. Non-parametric significance tests (e.g.
Kolmogorov-Smirnov test) are available in this scenario. 

If that data were available, I would prefer $Z$, and if not, I would use $Z^2$.


## Problem 2
*In this problem, you will compare genetics of two traits using the polygenic
risk score approach we discussed in the class. You are given the genotype and
phenotype data of two traits. For simplicity, the data has only 4 SNPs, and
they are considered to be independent.*

#3# (a)
Calculate the effect size of all SNPs in trait 1.

```{r,message=F}
# Load data
trait1 <- read_tsv('trait1.txt')
trait2 <- read_tsv('trait2-1.txt')
```

```{r}
# Regress phenotype on genotype for each SNP
b1 <- trait1 %>%
  gather(SNP, Genotype, 3:6) %>%
  group_by(SNP) %>%
  do(m = lm(phenotype ~ Genotype, data=.)) %>%
  mutate(B = summary(m)$coeff[2]) %>%
  select(-m)
b1
```


## (b)
*Using the estimated effect sizes, compute polygenic risk score (PRS) of trait
1 in the samples of trait 2 (i.e. we predict the trait 1 phenotype in these
samples).*

```{r}
prs<-as.matrix(trait2[,3:6]) %*% as.matrix(b1[,2], ncol=1)
head(prs)
```


### (c)
*Compute the correlation of the PRS with the phenotypes of trait 2.*

```{r}
cor(trait2$phenotype, prs)
```


## Problem 3
*We will answer some conceptual questions about annotating functions of SNPs:*

### (a)
*Evolutionary constraint across species (e.g. PhastCons or GERP) is often used
to define functional sequences. For a researcher studying human cognitive
trait, what do you think is the limitation of using this annotation?*

Many features of human cognition are unique to humans, so the genetic basis for
those features may be unique, too. Searching for conserved genes across species
will not reveal them.


### (b)
*Suppose we have a large number of whole-genome sequences of human, can you use
this dataset to improve annotation of functional non-coding sequences? You just
need to explain some general ideas.*

The dataset alone can be used to augment existing knowledge of non-coding
sequences. Large sample sizes can reveal rare variants in the population that
can then be used to compute position-weight matrices for sequence motifs, and
non-coding regions conserved across samples imply (but do not necessarily
prove) functional significance, including in non-coding regions.


### (c)
*We have enhancer data (e.g. ATAC-seq) in a tissue related to disease of
interest. In addition, we have a list of transcription factors (TFs) likely
important in this tissue from prior studies and we know the motifs of all these
TFs. Can you propose a better annotation combining enhancer data with TF
information? (Hint: most variants, even within an enhancer, are likely not
functional.)*

The sequences embedded in open chromatin (ATAC-seq) can be matched to TF motifs
from the set of relevant transcription factors. The prior information
essentially filters out the many sites in the enhancer data. Additional data,
such as methylation patterns and bound protein profiles, can also be used to
classify whether the enhancers of interest are active or inactive in diseased
individuals. ChromHMM is one tool for carrying out this analysis.

Also, should any motifs corresponding to TFs previously believed to be relevant
to the phenotype not show up in the ATAC-seq enhancer data, it would reduce the
confidence that it is indeed an important factor. The converse may be true but
is more difficult an argument to make due to the likelihood of false positives.
In Bayesian terms, the list of relevant TFs represents a prior model for
association between genotype and disease risk that is updated with enhancer
data to generate a posterior distribution of association and risk.


### (d)
*Now suppose we also have experimental data that identified several thousand of
SNPs with likely regulatory effects (e.g. from large-scale reporter assay). Can
you use this dataset to provide an even better functional annotation? (Hint:
use Supervised Learning.)*

A supervised learning algorithm can be used to determine the genetic/epigenetic
features characteristic of particular functional classes based on a training
dataset of genomes with SNPs of known regulatory function and/or pathogenic
effects. The particular classes of interest are determined by the modeller -
they can be biophysical (e.g. binding affinity for transcription factors),
sequence-based (e.g. likelihoods of genotypes from multiple sequence
alignments), enrichment of annotations in causal variants, among other things.
These algorithms should also take care to account for linkage, allele
frequencies, overfitting, and other confounding factors that can produce erroneous
classifications or non-generalizable models. Once the learning algorithm
constructs a model that faithfully recapitulates the training set, the model
can then be applied to the experimental data to categorize SNPs into the
functional classes, such as the regulatory pathway, whether they remove/augment
function, etc.


# Lab 6

## Problem 1

*Why are each of the above four input files important for running an analysis
with Matrix eQTL? (Hint: What is the goal of Matrix eQTL? Are there categories
for the results?)*

`gencode.v12.annotation.gtf.gz` - annotation file containing a list of
demonstrated features for genes in the entire genome

`geuvadis.annot.txt.gz` - SNP annotation information for transcripts (RNA);
provides information regarding identity, etc. of a SNP used for building
regression models

`GD462.GeneQuantRPKM.50FN.samplename.resk10.txt.gz` - normalized gene
expression data, used to build a regression of genotype (SNP) into a linear
model for gene expression

`geuvadis.snps.txt.gz` - genotype data, used once the linear model is built to
predict gene expression based on a regression model


## Problem 2

*If we were looking at all gene-SNP pairs, this analysis would take about 10
hours to complete. Obviously, that is unfeasible for this lab and we had to
limit our scope. We could have either limited the number of genes or the number
of SNPs we examine for this lab exercise. Using your knowledge of R (and
abilities to search R documentation), describe how you can determine which it
was. Did we limit the number of genes or SNPs? (Hint: look at the sections
labeled '# snp' and '# gene expression' in the `prepare_inputs_for_matrix-eqtl.r`
file).*

Limit the number of genes analyzed by selecting the first 20 rows in the
gene\_expression matrix.

```{r, eval=F}
gene_expression <- gene_expression[1:20, ]
```


## Problem 3

*Using the output files generated from the script above, how many genes are we
examining for our analysis? How many SNPs? What's our sample size?*

Number of genes: 20
```{bash, eval=F}
wc -l geuvadis_expression_data.txt
```

Number of samples: 373
```{bash, eval=F}
awk '{print NF}' geuvadis_expression_data.txt | sort -nu | tail -n 1
```

Number of SNPs: 3167497
```{bash, eval=F}
wc -l geuvadis_snp_location_data.txt
```


## Problem 4

*How many eQTLs were identified for the gene AFAP1L2? Provide the script you
used to determine this (consult previous lab commands for assistance with this,
if necessary).*

```{bash, eval=F}
grep -Rn AFAP1L2 | wc -l
```

Number of eQTLs: 3722


## Problem 5

*Provide the qqplot created from the script. Looking at our output files, how
many cis-eQTLs vs trans-eQTLs did we find in our analysis (combined across all
genes)? Is this what we might naively expect? What could result in such a high
number of QTLs? Hints: 1) Only basic QC has been applied to SNPs used in the
analysis (see Plink lab section 1.5.1 for more) & 2) Open up the output files
and compare the statstics for groups of QTLs.*

Number cis-eQTLs: 1

Number trans-eQTL: 10591

The elevation of the latter is to be expected given that there are only a few
kb where cis eQTLs can reside and the rest of the entire genome where
trans-eQTLs can be.

Many of the trans-eQTLs have identical summary statistics, suggesting that many
of them are in linkage disequilibrium with one other. SNPs within a linkage
block with have the same summary statistics with respect to an SNP outside the
block.

![qqplot](results/eqtls/gevadis_eQTL_QQ-plot.png)\


## Problem 6

*Examining the `prepare_inputs_for_predixcan.r` script and seeing its resulting
files, what do you think the section "# prepare genotype files (split 1 to 22)"
is doing? Why might it be important (or at least efficient) to do this?*

The genotypes are being split among the 22 chromosomes into separate files.
Splitting the data could make the process more amenable to parallelization,
given that the regressions/imputations for the SNPs can be computed
independently of one another. Another advantage could that separate files with
independent data can require less memory at runtime by loading one chromosome
into memory at a time rather than the full genome at once.


## Problem 7

*Examining the run_predixcan bash script, we can see that it calls for the
chromosome and sample files that we created in section 2.1. However, it also
calls for another file in the /datasets directory called
"gtex_v7_Cells_EBV-transformed_lymphocytes_imputed_europeans_tw_0.5_signif.db".
Given your knowledge of what the other input files are and of the PrediXcan
method in general, what do you think this new file is and why is it important?*

PrediXcan uses a reference transcriptome to train a prediction model for
genotype and expression. The regression model is used to impute the expression
data from individual genotypes.

The file likely contains expression data from lymphocytes gathered from a
European sample population by the GTEx project.


## Problem 8

*Similar to our exercise with Matrix eQTL, we are not creating correlations for
all genes in the genome due to time and storage space concerns. What line in
the **run_correlation.R** script limits the number of genes we're doing
correlations on? How many genes are we observing correlations for in this lab?*


```{r,eval=F}
filter (V3=='gene') %>% filter(V1 != 'chrX' & V1 != 'chrY' & V1 != 'chrM') %>%
```

Filter the data so that only genes that are not in the X, Y, or mitochondrial
chromosomes are considered. Then,

```{r,eval=F}
genes <- unique(joined$gene_symbol)[1:18]
```

Consider only the first 18 genes.


## Problem 9

*What gene demonstrates the greatest amount of correlation between observed and
predicted gene expression in our sample? Provide the correlation plot for it.*

```{r}
df <- read.table('results/correlations/summary_correlation.csv', header=T, sep=',')
head(df[order(-df$r2),])
```

MAN2B2 has the strongest correlation, r=0.15094139 with p-value=3.247202e-11.

![MAN2B2](results/correlations/MAN2B2.png)
